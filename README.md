Retail Probability of Default (PD) Scorecard Model

Dataset: Home Credit Default Risk

ğŸ“Œ Project Overview

This project presents the development of an end-to-end Retail Application Probability of Default (PD) Scorecard Model using traditional credit risk modeling techniques.

The objective was to build an interpretable, production-style scorecard aligned with banking standards for underwriting and risk-based decisioning.

The model estimates borrower-level default probability and converts it into an operational credit score using standard score scaling methodology.

ğŸ¯ Business Objective

To develop a regulatory-compliant, interpretable retail credit scorecard capable of:

Predicting Probability of Default (PD)

Ranking applicants by risk

Supporting approval strategy decisions

Enabling portfolio segmentation

This project follows the classical WOEâ€“IVâ€“Logistic Regression scorecard development framework widely used in banking institutions.

ğŸ—ï¸ Modeling Framework
1ï¸âƒ£ Data Preparation

Data cleaning and preprocessing

Trainâ€“test split (80% / 20%)

Missing value imputation

All transformations performed on training data only to prevent data leakage

2ï¸âƒ£ Feature Engineering

Weight of Evidence (WOE) binning using quantile segmentation (pd.qcut())

Stored:

Bin edges

WOE mappings

Information Value (IV) calculation

Variable selection threshold: IV â‰¥ 0.05

3ï¸âƒ£ Model Development
Single Factor Analysis (SFA)

Evaluated individual predictive strength

Verified directional consistency

Multi-Factor Analysis (MFA)

Logistic Regression using statsmodels

Removed variables with sign inconsistency

Checked multicollinearity (VIF < 2)

Final model: 9 variables

ğŸ“Š Model Performance
Metric	Train	Test
AUC	~0.73	~0.73
KS	~0.34	~0.34
Somerâ€™s D	~0.46	~0.46

No significant overfitting observed

Stable discrimination across datasets

ğŸ¯ Scorecard Scaling

Score scaling performed using industry-standard methodology:

Base Score: 600

Points to Double the Odds (PDO): 20

Base Odds: 50

Generated borrower-level credit scores for both train and test datasets.

ğŸ“ˆ Post-Model Analysis

Decile segmentation using train-based cutoffs

Applied identical bins to test dataset

Created Grade Bands (Aâ€“E)

Verified:

Monotonic bad rate behavior

Trainâ€“test stability

Conducted approval strategy analysis

ğŸ“‚ Repository Structure
Retail-PD-Scorecard/
â”‚
â”œâ”€â”€ notebooks/
â”‚   â”œâ”€â”€ 01_data_preprocessing.ipynb
â”‚   â”œâ”€â”€ 02_woe_iv_binning.ipynb
â”‚   â”œâ”€â”€ 03_model_building.ipynb
â”‚   â”œâ”€â”€ 04_validation.ipynb
â”‚   â”œâ”€â”€ 05_score_scaling.ipynb
â”‚   â”œâ”€â”€ 06_strategy_analysis.ipynb
â”‚
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ woe.py
â”‚   â”œâ”€â”€ model.py
â”‚   â”œâ”€â”€ scoring.py
â”‚   â”œâ”€â”€ validation.py
â”‚
â”œâ”€â”€ reports/
â”‚   â””â”€â”€ Retail_PD_Scorecard_Summary.pdf
â”‚
â””â”€â”€ README.md
ğŸ› ï¸ Tech Stack

Python

pandas

numpy

statsmodels

scikit-learn

matplotlib

seaborn

ğŸ“Œ Key Highlights

End-to-end credit risk scorecard development

WOEâ€“IV framework implementation

Multicollinearity control (VIF < 2)

Score scaling implementation (PDO methodology)

Stability validation and strategy simulation

Production-style train/test transformation logic

ğŸš€ How to Reproduce

Clone the repository

Install dependencies:

pip install -r requirements.txt

Run notebooks sequentially

Review model outputs and validation metrics

ğŸ“Œ Future Enhancements

Portfolio Stability Index (PSI) monitoring

Reject inference framework

Cross-validation enhancement

Model calibration assessment

Packaging into deployable scoring API

ğŸ“¬ Author

Pranjal Saxena
Credit Risk & Backend-Focused Developer
India
